# shellcheck disable=all

# =============================================================================
# API Connection
# =============================================================================
BUUN_CURATOR_API_URL=http://localhost:3000
INTERNAL_API_TOKEN=

# =============================================================================
# Temporal (Workflow Engine)
# =============================================================================
TEMPORAL_HOST=localhost:7233
TEMPORAL_NAMESPACE=default
TEMPORAL_TASK_QUEUE=buun-curator

# =============================================================================
# Worker Configuration
# =============================================================================
LOG_LEVEL=info
FEED_INGESTION_CONCURRENCY=5
MAX_ENTRY_AGE_DAYS=7

# Worker concurrency limits (0 = Temporal default)
# MAX_CONCURRENT_ACTIVITIES=10
# MAX_CONCURRENT_WORKFLOW_TASKS=10
# MAX_CONCURRENT_LOCAL_ACTIVITIES=0

# Health check port (for Kubernetes probes)
# HEALTH_PORT=8080

# =============================================================================
# Database (PostgreSQL)
# =============================================================================
DATABASE_URL=postgresql://user:password@localhost:5432/buun_curator

# =============================================================================
# OpenAI / LLM
# =============================================================================
OPENAI_API_KEY=sk-xxx
# OPENAI_BASE_URL=http://localhost:4000  # Optional: LiteLLM proxy or compatible endpoint

# LLM Models
# See: https://docs.langchain.com/oss/python/integrations/chat/anthropic#structured-output
LLM_MODEL=gpt-4o

# Task-specific models (if empty, falls back to LLM_MODEL)
# EXTRACTION_LLM_MODEL=    # Context extraction - Requires Structured Output
# REASONING_LLM_MODEL=     # GitHub reranking - Requires Structured Output
# SUMMARIZATION_LLM_MODEL= # Entry summarization - Any model works

# =============================================================================
# Meilisearch (Full-text Search)
# =============================================================================
MEILISEARCH_HOST=http://localhost:7700
MEILISEARCH_INDEX=buun-curator
# MEILISEARCH_API_KEY=  # Optional: API key for Meilisearch

# =============================================================================
# Translation
# =============================================================================
TRANSLATION_PROVIDER=microsoft  # "microsoft" or "deepl"

# Microsoft Translator
MS_TRANSLATOR_SUBSCRIPTION_KEY=
MS_TRANSLATOR_REGION=

# DeepL
# DEEPL_API_KEY=

# =============================================================================
# S3 / MinIO (for thumbnail storage)
# =============================================================================
# For AWS S3: leave S3_ENDPOINT empty, set S3_REGION
# For MinIO: set S3_ENDPOINT to MinIO URL
# S3_ENDPOINT=http://localhost:9000
# S3_ACCESS_KEY=
# S3_SECRET_KEY=
# S3_BUCKET=buun-curator
# S3_PREFIX=public
# S3_PUBLIC_URL=http://localhost:9000/buun-curator/public
# S3_REGION=us-east-1

# =============================================================================
# GitHub (for enrichment)
# =============================================================================
# GITHUB_TOKEN=  # Optional: For GitHub repo content fetching and reranking

# =============================================================================
# AI Evaluation (RAGAS)
# When enabled, requires LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY
# =============================================================================
# AI_EVALUATION_ENABLED=false
# EVALUATION_EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# =============================================================================
# Langfuse (LLM Observability)
# Required when AI_EVALUATION_ENABLED=true
# =============================================================================
# LANGFUSE_HOST=https://cloud.langfuse.com
# LANGFUSE_PUBLIC_KEY=
# LANGFUSE_SECRET_KEY=

# =============================================================================
# GraphRAG (Experimental) - Optional
# =============================================================================
# GRAPH_RAG_BACKEND=lightrag  # "lightrag" or "graphiti"
# GRAPH_RAG_LLM_MODEL=gpt-4o

# Embedding configuration
# GRAPHRAG_EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# GRAPHRAG_EMBEDDING_DIMENSIONS=768
# GRAPHRAG_EMBEDDING_THREADS=1

# LightRAG configuration
# LIGHTRAG_WORKING_DIR=/data/lightrag
# LIGHTRAG_QUERY_MODE=hybrid
# LIGHTRAG_LOG_LEVEL=info

# Memgraph (for LightRAG)
# MEMGRAPH_HOST=localhost
# MEMGRAPH_PORT=7687
# MEMGRAPH_USERNAME=
# MEMGRAPH_PASSWORD=
# MEMGRAPH_DATABASE=

# FalkorDB (for Graphiti)
# FALKORDB_HOST=localhost
# FALKORDB_PORT=6379
# FALKORDB_USERNAME=
# FALKORDB_PASSWORD=
# FALKORDB_GRAPH=

# =============================================================================
# OpenTelemetry Tracing - Optional
# =============================================================================
# OTEL_TRACING_ENABLED=false
# OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo.monitoring:4317
# OTEL_SERVICE_NAME=buun-curator-worker
# OTEL_EXPORTER_OTLP_INSECURE=true
