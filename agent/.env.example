# shellcheck disable=all

# =============================================================================
# Server Configuration
# =============================================================================
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=info
ENVIRONMENT=development

# =============================================================================
# API Connection (Next.js backend)
# =============================================================================
API_BASE_URL=http://localhost:3000
INTERNAL_API_TOKEN=

# =============================================================================
# CORS Configuration
# =============================================================================
# CORS_ORIGINS=["http://localhost:3000"]

# =============================================================================
# OpenAI / LLM
# =============================================================================
OPENAI_API_KEY=sk-xxx
# OPENAI_BASE_URL=http://localhost:4000  # Optional: LiteLLM proxy or compatible endpoint

# Agent models
RESEARCH_MODEL=gpt-4o

# =============================================================================
# Temporal (for triggering evaluation workflows)
# =============================================================================
TEMPORAL_HOST=localhost:7233
TEMPORAL_NAMESPACE=default
TEMPORAL_TASK_QUEUE=buun-curator

# =============================================================================
# AI Evaluation
# When enabled, triggers RAGAS evaluation workflow after responses.
# Requires LANGFUSE credentials in worker for score recording.
# =============================================================================
# AI_EVALUATION_ENABLED=false

# =============================================================================
# OpenTelemetry Tracing
# Sends traces to Grafana Tempo for distributed tracing.
# =============================================================================
# OTEL_TRACING_ENABLED=false
# OTEL_SERVICE_NAME=buun-curator-agent
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# =============================================================================
# Langfuse (LLM Observability)
# Used for LLM tracing via LiteLLM. If not set, tracing is disabled.
# =============================================================================
# LANGFUSE_HOST=https://cloud.langfuse.com
# LANGFUSE_PUBLIC_KEY=
# LANGFUSE_SECRET_KEY=
